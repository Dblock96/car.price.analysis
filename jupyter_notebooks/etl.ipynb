{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# ETL- Car Price Analysis\n",
    "US Market (Kaggle: hellbuoy/car-price-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "Load and clean the car price dataset to prepare it for analysis.  \n",
    "This includes standardising column names, correcting brand typos, handling missing values, engineering useful features, encoding categorical variables, and performing basic validation checks.  \n",
    "The cleaned dataset will be ready for Exploratory Data Analysis (EDA) and documentation in the README.\n",
    "\n",
    "## Inputs\n",
    "\n",
    " **Primary dataset**: `data/CarPrice_Assignment.csv` — contains information on car specifications and pricing in the US market.\n",
    "- **Data fields**: Includes `CarName`, `fueltype`, `aspiration`, `doornumber`, `carbody`, `drivewheel`, `enginelocation`, `enginesize`, `horsepower`, `citympg`, `highwaympg`, `price`, and other technical attributes.\n",
    "- **Tools & libraries**: Python 3.x with `pandas`, `numpy`, and `matplotlib` installed.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Cleaned DataFrame (`df_enc`) with standardised columns, corrected brand names, engineered features, and one-hot encoded categoricals.\n",
    "- Optional cleaned CSV saved as `data/car_price_clean.csv`.\n",
    "- Basic validation checks confirming data integrity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1 content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Section 1 - Extract & Inspect\n",
    "Load the CSV, confirm shape, peek at columns & missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Correct path to the CSV file inside the 'data' folder\n",
    "file_path = 'data/CarPrice_Assignment.csv'\n",
    "\n",
    "# Check if file exists in the 'data' directory\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"❌ File not found. Files in 'data' folder:\")\n",
    "    print(os.listdir('data'))  # Optional: show what's inside 'data' folder\n",
    "else:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "IMG_PATH  = Path(\"images\")\n",
    "\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "IMG_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_file = DATA_PATH / \"CarPrice_Assignment.csv\"\n",
    "assert input_file.exists(), f\"Missing file: {input_file}. Place it under data/.\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column info & dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values snapshot\n",
    "df.isna().sum().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFQo3ycuO-v6"
   },
   "source": [
    "# Section 2- Transform\n",
    "Steps:\n",
    "- Standardise column names\n",
    "- Split `CarName` → `make` / `model`\n",
    "- Basic NA handling on key fields\n",
    "- Clean brand typos (common in this dataset)\n",
    "- Feature engineering (e.g., price_per_cc, power_to_weight, mpg_ratio)\n",
    "- One-hot encode selected categoricals\n",
    "- Light outlier clipping for price (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Standardise column names: lowercase + underscores\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(\" \", \"_\")\n",
    "      .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "# 2) Split `carname` to `make` / `model` if present\n",
    "if \"carname\" in df.columns:\n",
    "    split_make_model = df[\"carname\"].str.split(\" \", n=1, expand=True)\n",
    "    df[\"make\"]  = split_make_model[0].str.lower()\n",
    "    df[\"model\"] = split_make_model[1].str.lower()\n",
    "else:\n",
    "    print(\"Column 'carname' not found; skipping make/model split.\")\n",
    "\n",
    "# 3) Ensure critical fields exist, then handle NAs\n",
    "critical_cols = [\"price\", \"enginesize\"]\n",
    "for c in critical_cols:\n",
    "    if c not in df.columns:\n",
    "        raise KeyError(f\"Expected column '{c}' not found in dataset.\")\n",
    "\n",
    "before_rows = len(df)\n",
    "df = df.dropna(subset=critical_cols)\n",
    "after_rows = len(df)\n",
    "print(f\"Dropped {before_rows - after_rows} rows with missing critical values.\")\n",
    "\n",
    "# 4) Clean common brand typos in 'make'\n",
    "brand_fix = {\n",
    "    \"maxda\": \"mazda\",\n",
    "    \"toyouta\": \"toyota\",\n",
    "    \"vokswagen\": \"volkswagen\",\n",
    "    \"vw\": \"volkswagen\",\n",
    "    \"porcshe\": \"porsche\",\n",
    "}\n",
    "if \"make\" in df.columns:\n",
    "    df[\"make\"] = df[\"make\"].replace(brand_fix)\n",
    "\n",
    "# 5) Feature engineering\n",
    "# - price_per_cc: price per engine cc\n",
    "# - power_to_weight: horsepower per curbweight\n",
    "# - mpg_ratio: highway / city mpg (efficiency balance)\n",
    "df[\"price_per_cc\"] = df[\"price\"] / df[\"enginesize\"]\n",
    "\n",
    "if {\"horsepower\", \"curbweight\"}.issubset(df.columns):\n",
    "    df[\"power_to_weight\"] = df[\"horsepower\"] / df[\"curbweight\"].replace(0, np.nan)\n",
    "\n",
    "if {\"highwaympg\", \"citympg\"}.issubset(df.columns):\n",
    "    df[\"mpg_ratio\"] = df[\"highwaympg\"] / df[\"citympg\"].replace(0, np.nan)\n",
    "\n",
    "# 6) Optional: clip extreme price outliers (1st–99th percentile)\n",
    "low, high = df[\"price\"].quantile([0.01, 0.99])\n",
    "df[\"price_clipped\"] = df[\"price\"].clip(lower=low, upper=high)\n",
    "\n",
    "# 7) One-hot encode selected categoricals\n",
    "categoricals = [c for c in [\n",
    "    \"fueltype\", \"aspiration\", \"doornumber\", \"carbody\",\n",
    "    \"drivewheel\", \"enginelocation\", \"enginetype\",\n",
    "    \"cylindernumber\", \"fuelsystem\", \"make\"\n",
    "] if c in df.columns]\n",
    "\n",
    "df_enc = pd.get_dummies(df, columns=categoricals, drop_first=True)\n",
    "\n",
    "print(\"Encoded shape:\", df_enc.shape)\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n",
    "Sanity checks: no negative prices, positive engine sizes, and basic NA review post-transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price should be non-negative\n",
    "assert df_enc[\"price\"].ge(0).all(), \"Found negative prices.\"\n",
    "\n",
    "# Enginesize should be positive\n",
    "assert df_enc[\"enginesize\"].gt(0).all(), \"Found non-positive enginesize.\"\n",
    "\n",
    "# Quick NA check after transforms/encodings\n",
    "na_counts = df_enc.isna().sum().sum()\n",
    "print(f\"Total remaining NA values across all columns: {na_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final NA Cleanup\n",
    "\n",
    "After all transformations, we found a small number of missing values remaining in the processed dataset.  \n",
    "For a fully clean dataset, we will remove any rows containing NA values.  \n",
    "Alternatively, you can fill missing values with a default (e.g., 0 or the column mean), depending on your analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing values in the processed DataFrame\n",
    "df_enc = df_enc.dropna()\n",
    "print(f\"Total remaining NA values across all columns: {df_enc.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any remaining missing values with 0\n",
    "df_enc = df_enc.fillna(0)\n",
    "print(f\"Total remaining NA values across all columns: {df_enc.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3- Load and Save Cleaned Data\n",
    "If you’d like a cleaned copy for modelling/visuals, save it back into the **same `data/` folder**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = DATA_PATH / \"car_price_clean.csv\"\n",
    "df_enc.to_csv(output_file, index=False)\n",
    "print(f\"Saved processed dataset to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (ETL)\n",
    "\n",
    "- The dataset was successfully loaded from `data/CarPrice_Assignment.csv`.\n",
    "- Columns were standardised (lowercase, underscores), and `CarName` was split into `make` and `model`.\n",
    "- Critical fields (`price`, `enginesize`) were validated and rows with missing critical values were removed.\n",
    "- Common brand typos (e.g., `maxda`, `toyouta`, `vokswagen`, `porcshe`) were corrected.\n",
    "- Key engineered features were added:\n",
    "  - `price_per_cc` – price normalised by engine size\n",
    "  - `power_to_weight` – horsepower per curb weight (when available)\n",
    "  - `mpg_ratio` – highway-to-city MPG ratio (when available)\n",
    "- Optional outlier clipping (1st–99th percentile) was applied to `price` for robust downstream analysis.\n",
    "- Selected categorical variables were one-hot encoded, producing a clean numeric table (`df_enc`) appropriate for visualisation and modelling.\n",
    "- Basic data-quality checks passed (non-negative prices, positive engine sizes; NA counts reviewed).\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**\n",
    "   - Perform in-depth visual exploration of the dataset using both static and interactive charts.\n",
    "   - Identify key relationships between features and price.\n",
    "   - Detect additional data anomalies or unusual patterns.\n",
    "\n",
    "2. **README Documentation**\n",
    "   - Update the project README to include:\n",
    "     - Project background and business context.\n",
    "     - ETL process summary.\n",
    "     - Key dataset insights.\n",
    "     - Instructions to reproduce the ETL process.\n",
    "   - Include images generated during ETL and EDA for better communication."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
